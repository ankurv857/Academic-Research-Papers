\section{Experiments and Results}
\label{sec:eval}
We use transactional data from instacart kaggle challenge to train all our models. As can 
be seen in Figure \ref{fig:sampledata} data has transactional details including consumer id, item id, 
order id, add to cart order, date of transaction, aisle id and department id.
Also, from Table 1, we can see that we utilize 1 year data which gets split into train, validation,
test1 and test2. We generate consumer-item-week level data with purchase/ non purchase being the target.
We use the above data to generate consumer-item purchase predictions for 2 time steps in the future (2 weeks in our case).
 \begin{figure*}[!t]
    \centering 
    \caption{Sample Dataset} 
    \includegraphics[width=6.6in]{img/sampledata.png} 
    \label{fig:sampledata} 
  \end{figure*}

  \begin{figure}[t]
    \centering 
    \caption{Most reordered Items across Departments} 
    \includegraphics[width=2in]{img/items.png} 
    \label{fig:items} 
  \end{figure}

  \begin{figure}[t]
    \centering 
    \caption{Density of consumers Vs. Basket Size} 
    \includegraphics[width=3in, height = 2in]{img/basket.png} 
    \\ {\scriptsize \bf Basket size between 5-8 has maximum consumer density}
    \label{fig:basket} 
  \end{figure}

  \begin{figure}[t]
    \centering 
    \caption{Reorder probability Vs. Add to cart order} 
    \includegraphics[width=3in , height = 2in]{img/addtocart.png} 
    \label{fig:addtocart} 
    \\ {\scriptsize \bf Probability of reordering decreases as the order of add to cart increases}
  \end{figure}

\begin{center}
\begin{table*}[!t]
\caption{BCELoss of Test2 for 12 Trials of Deep Learning Models} 
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{|r|l|r|r|r|r|r|r|r|}
  \hline
 {\bf Trial} & {\bf Optimizer} & {\bf Scheduler} & {\bf SWA} & {\bf Parameter Avg} &  {\bf MLP} & {\bf LSTM} 
 &  {\bf TCN} & {\bf TCN-LSTM} \\ [0.5ex] 
  \hline\hline
1 & RMSprop & ReduceLROnPlateau & True & False &  {\bf 0.0276} & 0.0306 & {\bf 0.0249} & 0.0307 \\ 
2 & RMSprop & CyclicLR & True & False &  0.0708 & {\bf 0.0269} & {\bf 0.0269} & 0.0348 \\ 
3 & Adam & ReduceLROnPlateau & True & False &  0.0295 & 0.0303 & 0.0667 & 0.0337 \\ 
4 & RMSprop & ReduceLROnPlateau & False & False &  0.0297 & {\bf 0.0275} & 0.0364 & 0.0759 \\ 
5 & RMSprop & CyclicLR & False & False &  {\bf 0.0250} & 0.0306 & 0.0600 & {\bf 0.0286} \\ 
6 & Adam & ReduceLROnPlateau & False & False&  0.0360 & {\bf 0.0302} & 0.0590 & 0.0309 \\ 
7 & RMSprop & ReduceLROnPlateau & False & True &  0.0293 & 0.0432 & 0.0453 & 0.0381 \\ 
8 & RMSprop & CyclicLR& False & True &  {\bf 0.0245} & 0.0378 & 0.0569 & {\bf 0.0262} \\ 
9 & Adam & ReduceLROnPlateau & False & True & 0.0700 & 0.0491 & 0.0610 & 0.0382 \\ 
10 & RMSprop & ReduceLROnPlateau & True & True & 0.0356 & 0.0364 & {\bf 0.0238} & 0.0309 \\ 
11 & RMSprop & CyclicLR & True & True &  0.0420 & 0.0377 & 0.0284 & {\bf 0.0269} \\ 
12 & Adam  & ReduceLROnPlateau & True & True&  0.0321 & 0.0306 & 0.0547 & 0.0305 \\ [1ex] 
   \hline
\end{tabular}}
\label{tab:dlmodels}
\end{table*} 
\end{center}

\begin{table}[t]
\caption{BCELoss of Test2 for 6 best Trials of ML Models}
\vspace{0.1 in}
\centering
\resizebox{3.3in}{!}
{%
\begin{tabular}{|c|c|c|c|c|}
\hline
{\bf Trial} & {\bf Hyper-Parameter} & {\bf Xgboost} & {\bf RandomForest} \\  
\hline\hline
1  		&  HyperOpt &  {\bf 0.0332} &  0.0526   \\ 
2	  		&  HyperOpt &  0.0364 &  0.0479   \\ 
3  		&  HyperOpt &  0.0347 &  {\bf 0.0416}  \\ 
4	  		&  HyperOpt &  0.0364 &  {\bf 0.0449}  \\ 
5	  		&  HyperOpt &  {\bf 0.0335} &  {\bf 0.0459}  \\ 
6	  		&  HyperOpt &  {\bf 0.0339} &  0.0578  \\ 
\hline
\end{tabular}
}
\label{tab:mlmodels}
\end{table}


\begin{table}[t]
\caption{ BCELoss mean of top 3 trials across data splits}
\vspace{0.1 in}
\centering
\resizebox{3.3in}{!}
{%
\begin{tabular}{|c|c|c|c|c|}
\hline
{\bf Model Type} & {\bf Val BCELoss} & {\bf Test1 BCELoss} & {\bf Test2 BCELoss} \\ 
\hline\hline 
MLP	  		&  0.0405 &  0.0289 &  0.0256  \\ \hline
LSTM  		&  0.0373 &  0.0293 &  0.0282 \\ \hline
{\bf TCN}			&  {\bf 0.0368}  &  {\bf 0.0292} &  {\bf 0.0251}  \\ \hline
TCNLSTM 	& 0.0368  & 0.0304	& 0.0273	 \\ \hline
Xgboost 	& 0.0352 & 0.0318	& 0.0335	\\ \hline
RandomForest & 0.0437 & 0.0389	& 0.0441	\\ \hline
\end{tabular}
}
\label{tab:training}
\end{table}

\begin{table}[t]
\caption{ Stacked Generalization Results}
\vspace{0.1 in}
\centering
\resizebox{3.3in}{!}
{%
\begin{tabular}{|c|c|c|c|c|c|}
\hline
{\bf Model Type} & {\bf K Value} & {\bf Val BCELoss} & {\bf Test1 BCELoss} & {\bf Test2 BCELoss} \\ 
\hline\hline 
{\bf Weighted K Best}	  &  {\bf 3}  &  {\bf 0.0386} &  {\bf 0.0278} &  {\bf 0.0242}  \\ \hline
Weighted K Best	  		&  5  &  0.0373 &  0.0282 &  0.0245  \\ \hline
Weighted K Best	  		 &  10 &  0.0397 &  0.0290 &  0.0258  \\ \hline
Weighted K Best	  		 &  15 &  0.0389 &  0.0296 &  0.0272  \\ \hline
Weighted K Best	  		&  25  &  0.0394 &  0.0316 &  0.0287  \\ \hline
\end{tabular}
}
\label{tab:stacking}
\end{table}

\begin{table}[hbt!]
\caption{Final Accuracy post F\textsubscript{1}-Maximization}
\vspace{0.1 in}
\centering
\resizebox{2.5in}{!}
{%
\begin{tabular}{|c|c|c|c|}
\hline
{\bf Data Split} & {\bf Precision} & {\bf Recall} & {\bf F\textsubscript{1}-Score} \\ 
\hline\hline 
Validation	  	 &  0.3401 &  0.4981 &  0.4042  \\ \hline
Test1	  		 &  0.3323 &  0.5103 &  0.4024  \\ \hline
Test2	  		 & 0.3506 &  0.4964 &  0.4109 \\ \hline
\end{tabular}
}
\label{tab:Fscore}
\end{table}

  \begin{figure}[hbt!]
    \centering 
    \caption{Probability Distributions} 
    \includegraphics[width=2in]{img/density.png} 
    \\ {\scriptsize \bf High density probability zone for the non purchased cases lies between 0 and 
    0.1, whereas for the purchased cases it lies between 0.25 and 0.35}
    \label{fig:probdensity} 
  \end{figure}
\subsection{Experiment Setups}
We started with exploratary data analysis, looking at the data from various cuts and 
trying to study the variations of the features with target. Some of them include looking at the 
density of consumers with different basket sizes Figure~\ref{fig:basket}, orders placed for items 
across departments Figure \ref{fig:items}, variation of reorder probability with add to cart order Figure \ref{fig:addtocart},
order probability variations at different temporal cuts like week, month and quarter, transactional metrics 
like total orders, total reorders, recency, gap between orders, at both consumer and item levels.
We then performed multiple experiments with above features and different hyper-configurations to land at reasonable 
parameters to perform our experiments and present results.

\subsection{Results and Observations}
Tables \ref{tab:dlmodels} and \ref{tab:mlmodels} show the experimental results obtained across models with different 
hyper-configurations. Table \ref{tab:dlmodels} contains the Deep Learning Experiment setup results
and Table \ref{tab:mlmodels} has Machine Learning model results. From model performance perspective, it is observed that 
TCN showed the best Accuracy score at test2, with most of the other Deep Learning models having comparable scores.
Also, from Table \ref{tab:training} we see that all the Deep Learning models out perform Machine Learning models 
including Xgboost and RandomForest both in terms of Accuracy and Generalization. This table has the average of 
BCELoss across 3 best trials, and it can be observed that TCN has the lowest BCELoss, which translates into
best Accuracy. From hyper-parameter configuration perspective we observe that RMSprop and CyclicLR emerged as the 
clear winners as Optimizer and Scheduler respectively from Table \ref{tab:dlmodels}. 7 out of 12 times, we 
found this combination (out of 3 possible combinations) generating the best result.

We also present the effectiveness of combining predictions in the form of stacking. From Table \ref{tab:stacking}, we can see the 
results of stacking at different values of K, for Weighted K-Best model setups. We realised the best outcome at 
K = 3, which led to a better score compared to any individual model.
In Figure \ref{fig:probdensity}, we can see the probability distributions post stacking for both labels of the target.
Finally we apply F\textsubscript{1}-Maximization over stacked probability values so as to generate 
binary predictions(1/0 referring to Purchase/Non Purchase). F\textsubscript{1}-Score Optimizer helps strike a 
balance between Precision and Recall \cite{buckland1994relationship}. Post F\textsubscript{1}-Maximization we 
observe that Precision, Recall and F\textsubscript{1}-Score are close enough for all data splits, as can be seen 
in Table \ref{tab:Fscore}. We scored {\bf 0.4109} over unseen data (Test2)
from Table \ref{tab:Fscore} as F\textsubscript{1}-Score with the described framework.
\subsection{Industrial Applications}
The Next Logical Purchase framework has multiple applications in retail/e-retail industry. Some of its applications 
include
\begin{itemize}
\item {\bf Personalized Marketing:} With prior knowledge of next logical purchase, accurate item recommendations and 
optimal offer rollouts can be made at consumer level to provide a seamless and delightful consumer experience.
\item {\bf Inventory Planning:} Short Term Inventory Planning (2-4 weeks) is largely dependant over 
consumer preference in near future. Solution of the current problem can assist planners in better inventory planning.
\item {\bf Assortment Planning:} In retail stores, consumer choice study can be used to optimize the store 
layout with right product placement over right shelf.
\end{itemize}
