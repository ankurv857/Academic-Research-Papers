\section{Experiments and Results}
\label{sec:eval}
We use transactional data from instacart kaggle challenge to train all our models (sample data \ref{fig:sampledata}). From 
sample data we can see that data contains transactional details including
order id, add to cart order, date of transaction, aisle id and department id for each consumer-item transaction.
As described in Table \ref{tab:datasplit}, we utilize 1 year data for each consumer-item combination, 
which then gets split into train, validation, test1 and test2 as per our validation strategy. 
We generate consumer-item-week level data with purchase/ non purchase being the target,
and use this data to train all our models.
 \begin{figure*}[!t]
    \centering 
    \caption{Sample Dataset} 
    \includegraphics[width=6.6in]{img/sampledata.png} 
    \label{fig:sampledata} 
  \end{figure*}

  \begin{figure}[t]
    \centering 
    \caption{Most reordered Items across Departments} 
    \includegraphics[width=2.25in]{img/items.png} 
    \label{fig:items} 
  \end{figure}

  \begin{figure}[t]
    \centering 
    \caption{Density of consumers Vs. Basket Size} 
    \includegraphics[width=3in, height = 2in]{img/basket.png} 
    \\ {\scriptsize \bf Basket size between 5-8 has maximum consumer density}
    \label{fig:basket} 
  \end{figure}

  \begin{figure}[t]
    \centering 
    \caption{Reorder probability Vs. Add to cart order} 
    \includegraphics[width=3in , height = 2in]{img/addtocart.png} 
    \label{fig:addtocart} 
    \\ {\scriptsize \bf Probability of reordering decreases as the order of add to cart increases}
  \end{figure}

\begin{center}
\begin{table*}[!t]
\caption{BCELoss of Test2 for 12 Trials of Deep Learning Models} 
\centering
\resizebox{\textwidth}{!}{\begin{tabular}{|r|l|r|r|r|r|r|r|r|}
  \hline
 {\bf Trial} & {\bf Optimizer} & {\bf Scheduler} & {\bf SWA} & {\bf Parameter Avg} &  {\bf MLP} & {\bf LSTM} 
 &  {\bf TCN} & {\bf TCN-LSTM} \\ [0.5ex] 
  \hline\hline
1 & RMSprop & ReduceLROnPlateau & True & False &  {\bf 0.0276} & 0.0306 & {\bf 0.0249} & 0.0307 \\ 
2 & RMSprop & CyclicLR & True & False &  0.0708 & {\bf 0.0269} & {\bf 0.0269} & 0.0348 \\ 
3 & Adam & ReduceLROnPlateau & True & False &  0.0295 & 0.0303 & 0.0667 & 0.0337 \\ 
4 & RMSprop & ReduceLROnPlateau & False & False &  0.0297 & {\bf 0.0275} & 0.0364 & 0.0759 \\ 
5 & RMSprop & CyclicLR & False & False &  {\bf 0.0250} & 0.0306 & 0.0600 & {\bf 0.0286} \\ 
6 & Adam & ReduceLROnPlateau & False & False&  0.0360 & {\bf 0.0302} & 0.0590 & 0.0309 \\ 
7 & RMSprop & ReduceLROnPlateau & False & True &  0.0293 & 0.0432 & 0.0453 & 0.0381 \\ 
8 & RMSprop & CyclicLR& False & True &  {\bf 0.0245} & 0.0378 & 0.0569 & {\bf 0.0262} \\ 
9 & Adam & ReduceLROnPlateau & False & True & 0.0700 & 0.0491 & 0.0610 & 0.0382 \\ 
10 & RMSprop & ReduceLROnPlateau & True & True & 0.0356 & 0.0364 & {\bf 0.0238} & 0.0309 \\ 
11 & RMSprop & CyclicLR & True & True &  0.0420 & 0.0377 & 0.0284 & {\bf 0.0269} \\ 
12 & Adam  & ReduceLROnPlateau & True & True&  0.0321 & 0.0306 & 0.0547 & 0.0305 \\ [1ex] 
   \hline
\end{tabular}}
\label{tab:dlmodels}
\end{table*} 
\end{center}

\begin{table}[t]
\caption{BCELoss of Test2 for 6 best Trials of ML Models}
\vspace{0.1 in}
\centering
\resizebox{3.3in}{!}
{%
\begin{tabular}{|c|c|c|c|c|}
\hline
{\bf Trial} & {\bf HyperParameter} & {\bf Xgboost} & {\bf RandomForest} \\  
\hline\hline
1  		&  HyperOpt &  {\bf 0.0332} &  0.0526   \\ 
2	  		&  HyperOpt &  0.0364 &  0.0479   \\ 
3  		&  HyperOpt &  0.0347 &  {\bf 0.0416}  \\ 
4	  		&  HyperOpt &  0.0364 &  {\bf 0.0449}  \\ 
5	  		&  HyperOpt &  {\bf 0.0335} &  {\bf 0.0459}  \\ 
6	  		&  HyperOpt &  {\bf 0.0339} &  0.0578  \\ 
\hline
\end{tabular}
}
\label{tab:mlmodels}
\end{table}


\begin{table}[t]
\caption{ BCELoss mean of top 3 trials across data splits}
\vspace{0.1 in}
\centering
\resizebox{3.3in}{!}
{%
\begin{tabular}{|c|c|c|c|c|}
\hline
{\bf Model Type} & {\bf Val BCELoss} & {\bf Test1 BCELoss} & {\bf Test2 BCELoss} \\ 
\hline\hline 
MLP	  		&  0.0405 &  0.0289 &  0.0256  \\ \hline
LSTM  		&  0.0373 &  0.0293 &  0.0282 \\ \hline
{\bf TCN}			&  {\bf 0.0368}  &  {\bf 0.0292} &  {\bf 0.0251}  \\ \hline
TCNLSTM 	& 0.0368  & 0.0304	& 0.0273	 \\ \hline
Xgboost 	& 0.0352 & 0.0318	& 0.0335	\\ \hline
RandomForest & 0.0437 & 0.0389	& 0.0441	\\ \hline
\end{tabular}
}
\label{tab:training}
\end{table}

\begin{table}[t]
\caption{ Stacked Generalization Results}
\vspace{0.1 in}
\centering
\resizebox{3.3in}{!}
{%
\begin{tabular}{|c|c|c|c|c|c|}
\hline
{\bf Model Type} & {\bf K Value} & {\bf Val BCELoss} & {\bf Test1 BCELoss} & {\bf Test2 BCELoss} \\ 
\hline\hline 
{\bf Weighted K Best}	  &  {\bf 3}  &  {\bf 0.0386} &  {\bf 0.0278} &  {\bf 0.0242}  \\ \hline
Weighted K Best	  		&  5  &  0.0373 &  0.0282 &  0.0245  \\ \hline
Weighted K Best	  		 &  10 &  0.0397 &  0.0290 &  0.0258  \\ \hline
Weighted K Best	  		 &  15 &  0.0389 &  0.0296 &  0.0272  \\ \hline
Weighted K Best	  		&  25  &  0.0394 &  0.0316 &  0.0287  \\ \hline
\end{tabular}
}
\label{tab:stacking}
\end{table}

\begin{table}[hbt!]
\caption{Final Accuracy post F\textsubscript{1}-Maximization}
\vspace{0.1 in}
\centering
\resizebox{2.5in}{!}
{%
\begin{tabular}{|c|c|c|c|}
\hline
{\bf Data Split} & {\bf Precision} & {\bf Recall} & {\bf F\textsubscript{1}-Score} \\ 
\hline\hline 
Validation	  	 &  0.3401 &  0.4981 &  0.4042  \\ \hline
Test1	  		 &  0.3323 &  0.5103 &  0.4024  \\ \hline
Test2	  		 & 0.3506 &  0.4964 &  0.4109 \\ \hline
\end{tabular}
}
\label{tab:Fscore}
\end{table}

  \begin{figure}[hbt!]
    \centering 
    \caption{Probability Distributions} 
    \includegraphics[width=2in]{img/density.png} 
    \\ {\scriptsize \bf High density probability zone for the non purchased cases lies between 0 and 
    0.1, whereas for the purchased cases it lies between 0.25 and 0.35}
    \label{fig:probdensity} 
  \end{figure}
\subsection{Experiment Setups}
We start with exploratary data analysis, looking at the data from various cuts. We
study the variations of different features with our target (purchase/ non purchase). Some of our studies are
density of consumers versus basket size (Figure~\ref{fig:basket}), reorder visualization of items 
across departments (Figure \ref{fig:items}), variation of reorder probability with add to cart order (Figure \ref{fig:addtocart}),
order probability variations at different temporal cuts like week, month and quarter, transactional metrics 
like total orders, total reorders, recency, gap between orders, at both consumer and item levels.
We then perform multiple experiments with the above mentioned features and different hyperparameter configurations to land at reasonable 
hyperparameters to perform final experiments and present our results.

\subsection{Results and Observations}
Tables \ref{tab:dlmodels} and \ref{tab:mlmodels} show the experimental results obtained across models with different 
hyperparameter configurations. Table \ref{tab:dlmodels} contains the Deep Learning Experiment setup results
and Table \ref{tab:mlmodels} has Machine Learning model results. From model performance perspective, it is observed that 
Temporal Convolution Network (TCN) has least average BCELoss of 0.0251, approximately 2\% better than the second best model
which is Multi Layer Perceptron (MLP) having average BCELoss of 0.0256. Table \ref{tab:training} presents
the comparative analysis of average scores across all models. Also, we observe in Table \ref{tab:training} 
that Deep Learning models out perform Machine Learning models including Xgboost and RandomForest in terms of accuracy.
Average BCELoss of Deep Learning model is approximately about 0.0266 , whereas for Machine Learning models its 
approximately about 0.0388. From hyperparameter configuration perspective, we observe that RMSprop and CyclicLR emerged as the 
winners as Optimizer and Scheduler respectively (from Table \ref{tab:dlmodels}). 7 out of 12 times, the 
combination of RMSprop and CyclicLR (out of 3 possible combinations) generate the best result.

We also present the effectiveness of combining submodel predictions and F\textsubscript{1}-Maximization. 
Table \ref{tab:stacking} outlines the results of stacking at different values of K for Weighted K-Best stacker model. 
We realise the best accuracy or least BCELoss of 0.0242 at K = 3.
To analyse our probability values post stacking, we plot the probability distributions for both labels of the target,
as can be seen in Figure \ref{fig:probdensity}.
Finally we apply F\textsubscript{1}-Maximization over stacked probability values so as to generate 
purchase predictions. F\textsubscript{1}-Score Optimizer helps strike
balance between Precision and Recall \cite{buckland1994relationship}. Post F\textsubscript{1}-Maximization we 
observe that Precision, Recall and F\textsubscript{1}-Score are close enough for all data splits, as can be seen 
in Table \ref{tab:Fscore}. F\textsubscript{1}-Score of our model over unseen data (test2) is 0.4109
(Table \ref{tab:Fscore}).
\subsection{Industrial Applications}
The Next Logical Purchase framework has multiple applications in retail/e-retail industry. Some of them 
include:
\begin{itemize}
\item {\bf Personalized Marketing:} With prior knowledge of next logical purchase, accurate item recommendations and 
optimal offer rollouts can be made at consumer level. This will enable a seamless and delightful consumer 
shopping experience.
\item {\bf Inventory Planning:} Consumer preference model can also be used in better short term inventory planning (2-4 weeks),
which is largely dependant over what consumer is going to purchase in the near future.
\item {\bf Assortment Planning:} In retail stores, consumer choice study can be used to optimize the store 
layout with right product placement over right shelf.
\end{itemize}
