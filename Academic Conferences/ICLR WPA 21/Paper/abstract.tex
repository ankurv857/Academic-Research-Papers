\begin{abstract}
We propose Weighted Parameter Averaging in Deep Neural Network, an algorithm to improve DNN accuracy and 
generalization. Our method is to average the neural network parameters of multiple local minima post training, weighted by 
validation loss of respective model checkpoint. The resulting model generalizes better 
than single best checkpoint based model over unseen data. We present the performance of Weighted Parameter 
Averaging in Deep Neural Network at multiple experimental setups. For our experiments we use different combinations of 
Optimizers and Schedulers over multiple datasets and present the resulting performance scores. We demonstrate the reduction 
in error and good generalization performance of the resulting models over our experimental datasets. We show the accuracy 
improvements obtained across multiple type of learning rate schedulers like CyclicLR, CosineAnnealingLR and ReduceLROnPlateau.
We also present the comparison of Weighted Parameter Averaging with Stochastic Weight Averaging in the form of accuracy.
\end{abstract}